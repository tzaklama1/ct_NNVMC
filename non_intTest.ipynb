{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422dc3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host: submit63.mit.edu\n",
      "Python: /work/submit/tzaklama/ct_NNVMC/venvs/ct_nnvmc_venv/bin/python\n",
      "CUDA_VISIBLE_DEVICES: 0\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, socket\n",
    "print(\"Host:\", socket.gethostname())\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.getenv(\"CUDA_VISIBLE_DEVICES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a5a67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host: submit63.mit.edu\n",
      "Python: /work/submit/tzaklama/ct_NNVMC/venvs/ct_nnvmc_venv/bin/python\n",
      "/dev/nvidia0  /dev/nvidia2  /dev/nvidiactl   /dev/nvidia-uvm-tools\n",
      "/dev/nvidia1  /dev/nvidia3  /dev/nvidia-uvm\n",
      "\n",
      "/dev/nvidia-caps:\n",
      "nvidia-cap1  nvidia-cap2\n",
      "GPU 0: NVIDIA GeForce GTX 1080 Ti (UUID: GPU-8cc96b65-f552-c8c5-ad52-03e6616b2931)\n"
     ]
    }
   ],
   "source": [
    "import os, socket, sys\n",
    "print(\"Host:\", socket.gethostname())      # MUST be c1234 (NOT submit66)\n",
    "print(\"Python:\", sys.executable)          # /work/.../ct_nnvmc_venv/bin/python\n",
    "!ls /dev/nvidia*                           # should list /dev/nvidia0, etc.\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2661ff5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or JAX\n",
    "import jax; jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f101f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_intTest.py\n",
    "\"\"\"\n",
    "Tests model for non-interacting 1d chain of fermions.\n",
    "\n",
    "1d non-int chain is calculated analytically for given configuration and then used as target for model.\n",
    "\"\"\"\n",
    "import jax, jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bbc824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate exact coefficients for non-interacting 1d chain of fermions\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def slater_coeffs_1d_sitebasis(L, N, bc=None):\n",
    "    \"\"\"Return (coeffs_complex, basis_masks) for the noninteracting ground state.\"\"\"\n",
    "    if bc is None:\n",
    "        bc = 'pbc' if (N % 2 == 1) else 'apbc'\n",
    "    phi = 0.0 if bc == 'pbc' else np.pi\n",
    "\n",
    "    # choose occupied momenta\n",
    "    if N % 2 == 0:\n",
    "        ns = np.arange(-N//2, N//2)\n",
    "    else:\n",
    "        ns = np.arange(-(N//2), N//2 + 1)\n",
    "    ks = (2*np.pi*ns + phi)/L\n",
    "\n",
    "    # all N-particle bitmasks (ascending order of positions)\n",
    "    from itertools import combinations\n",
    "    masks, positions = [], []\n",
    "    for occ in combinations(range(L), N):\n",
    "        masks.append(sum(1<<i for i in occ))\n",
    "        positions.append(np.array(occ, dtype=int))\n",
    "    positions = np.stack(positions)                          # (H, N)\n",
    "\n",
    "    # build orbital matrix Phi[a,x] = e^{ik_a x}/sqrt(L)\n",
    "    Phi = np.exp(1j*np.outer(ks, np.arange(L))) / np.sqrt(L) # (N, L)\n",
    "\n",
    "    # for each configuration, slice columns at positions and take det\n",
    "    coeffs = np.empty(len(masks), dtype=np.complex128)\n",
    "    for idx, pos in enumerate(positions):\n",
    "        coeffs[idx] = np.linalg.det(Phi[:, pos]) / np.sqrt(math.factorial(N))\n",
    "    return coeffs, np.array(masks, dtype=np.uint64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e024a74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/submit/tzaklama/ct_NNVMC\n",
      "Exact TB E0 = -4.82842712474619\n"
     ]
    }
   ],
   "source": [
    "## Calculate exact ground-state energy for non-interacting 1D chain of fermions for reference\n",
    "%cd /work/submit/tzaklama/ct_NNVMC\n",
    "import config as C\n",
    "def tb_E0_1d(L, N, t, bc=None):\n",
    "    \"\"\"\n",
    "    Exact ground-state energy for non-interacting 1D chain.\n",
    "    bc: 'pbc' or 'apbc' (auto-choose if None: pbc for odd N, apbc for even N)\n",
    "    \"\"\"\n",
    "    if bc is None:\n",
    "        bc = 'pbc' if (N % 2 == 1) else 'apbc'\n",
    "    phi = 0.0 if bc == 'pbc' else np.pi\n",
    "    # choose the N integers centered around 0\n",
    "    if N % 2 == 0:\n",
    "        ns = np.arange(-N//2, N//2)                # even N\n",
    "    else:\n",
    "        ns = np.arange(-(N//2), N//2 + 1)          # odd N\n",
    "    ks = (2*np.pi*ns + phi)/L\n",
    "    eps = -2*t*np.cos(ks)\n",
    "    return np.sum(np.sort(eps))  # already lowest; sort is harmless\n",
    "\n",
    "\n",
    "E_exact = tb_E0_1d(L=C.N_SITES, N=C.N_PART, t=2*C.T_HOP, bc='pbc')\n",
    "print(\"Exact TB E0 =\", E_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0356048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training states: 280 | Hamiltonians: 5\n",
      "JAX backend: GPU | lattice: 1d | N=8, Np=4\n",
      "epoch    0  loss = 5.4999e-01\n",
      "epoch  300  loss = 4.0871e-03\n",
      "epoch  600  loss = 1.5569e-05\n",
      "epoch  900  loss = 6.7625e-04\n",
      "epoch 1200  loss = 1.2515e-04\n",
      "epoch 1500  loss = 3.3379e-07\n",
      "epoch 1800  loss = 9.4113e-02\n",
      "epoch 2100  loss = 2.4992e-04\n",
      "epoch 2400  loss = 2.8610e-06\n",
      "epoch 2700  loss = 2.6502e-01\n",
      "epoch 3000  loss = 1.0524e-01\n",
      "epoch 3300  loss = 7.5177e-03\n",
      "\n",
      "First 10 coefficients for (t,V)=(0.5,0.0):\n",
      "|11000000⟩ → -2.09092 +0.13990 i\n",
      "|10100000⟩ → -3.42089 +1.80702 i\n",
      "|10010000⟩ → -3.29993 +3.88999 i\n",
      "|10001000⟩ → -1.73407 +5.24778 i\n",
      "|10000100⟩ → +0.37670 +5.09048 i\n",
      "|10000010⟩ → +1.72532 +3.50834 i\n",
      "|10000001⟩ → +1.58021 +1.36985 i\n",
      "|01100000⟩ → -1.10907 +1.72096 i\n",
      "|01010000⟩ → -1.21666 +3.70218 i\n",
      "|01001000⟩ → +0.39346 +5.08952 i\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Latest version of the code to train a Lattice-TransFormer on synthetic data for non-interacting fermions in 1D.\n",
    "\"\"\"\n",
    "import jax, jax.numpy as jnp, optax\n",
    "from flax.training import train_state\n",
    "import config as C\n",
    "T_LIST = [1.2, 1.0, 0.4, 0.6, 0.8]          # example\n",
    "V_LIST = [0.0, 0.0, 0.0, 0.0, 0.0]          # non-interacting, so V=0\n",
    "N_LIST = [2, 3, 5, 4, 4]          # particle numbers (can all differ)\n",
    "\n",
    "assert len(T_LIST)==len(V_LIST)==len(N_LIST)\n",
    "G = len(T_LIST)              # number of Hamiltonians\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2)  Build concatenated training set\n",
    "# ---------------------------------------------------------------------------\n",
    "OCC_ALL, LAM_ALL, TARGET_ALL, GID_ALL = [], [], [], []\n",
    "for gid,(t,v,npart) in enumerate(zip(T_LIST, V_LIST, N_LIST)):\n",
    "    # basis for this particle number --------------------\n",
    "    if C.LATTICE == \"1d\":\n",
    "        from phys_system.lattice1D import enumerate_fock, mask_to_array\n",
    "        basis = enumerate_fock(C.N_SITES, npart)\n",
    "        occ   = jnp.array([mask_to_array(m, C.N_SITES) for m in basis],\n",
    "                          dtype=jnp.int32)\n",
    "    else:\n",
    "        import phys_system.honeycomb as hc\n",
    "        basis = hc.enumerate_fock(C.N_SITES, npart)\n",
    "        occ   = jnp.array([hc.mask_to_array(m, C.N_SITES) for m in basis],\n",
    "                          dtype=jnp.int32)\n",
    "\n",
    "    # λ-vector extended to include N --------------------\n",
    "    lam_vec = jnp.array([t, v, npart], dtype=jnp.float32)\n",
    "    lam     = jnp.tile(lam_vec, (len(basis),1))\n",
    "\n",
    "    \n",
    "    # synthetic target coefficients replaced by exact non-interacting ones\n",
    "    #key  = jax.random.PRNGKey(C.SEED + gid)\n",
    "    #targ = jax.random.normal(key, (len(basis), 2))*0.1\n",
    "    coeffs, masks = slater_coeffs_1d_sitebasis(L=C.N_SITES, N=npart, bc='pbc')\n",
    "    # TARGET aligned to OCC (Re, Im)\n",
    "    targ = jnp.stack([jnp.array(coeffs.real), jnp.array(coeffs.imag)], axis=1)\n",
    "\n",
    "    gid_vec = jnp.full((len(basis),), gid, dtype=jnp.int32)\n",
    "\n",
    "    OCC_ALL.append(occ)\n",
    "    LAM_ALL.append(lam)\n",
    "    TARGET_ALL.append(targ)\n",
    "    GID_ALL.append(gid_vec)\n",
    "\n",
    "# concatenate everything --------------------------------\n",
    "OCC     = jnp.concatenate(OCC_ALL,    axis=0)\n",
    "LAM     = jnp.concatenate(LAM_ALL,    axis=0)   # shape (B,3)\n",
    "TARGET  = jnp.concatenate(TARGET_ALL, axis=0)\n",
    "GIDS    = jnp.concatenate(GID_ALL,    axis=0)   # shape (B,)\n",
    "\n",
    "print(\"Total training states:\", OCC.shape[0], \"| Hamiltonians:\", G)\n",
    "\n",
    "# ---------- model -----------------------------------------------------------\n",
    "from networks.model import LatticeTransFormer\n",
    "model = LatticeTransFormer(n_sites=C.N_SITES, depth=8, d_model=256)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3)  Loss & training step\n",
    "# ---------------------------------------------------------------------------\n",
    "from Loss.loss import overlap_loss_multi, amp_phase_loss, loss_normDiff   \n",
    "LOSS_TYPE = \"overlap_multi\"   # choose in config or set here\n",
    "\n",
    "def create_state(rng):\n",
    "    params = model.init(rng, OCC, LAM, train=False)\n",
    "    tx     = optax.adam(1e-3)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "def loss_fn(params):\n",
    "    preds = model.apply(params, OCC, LAM, train=False)\n",
    "    if LOSS_TYPE == \"overlap_multi\":\n",
    "        return overlap_loss_multi(preds, TARGET, GIDS, num_groups=G)\n",
    "    elif LOSS_TYPE == \"amp_phase\":\n",
    "        return amp_phase_loss(preds, TARGET, neighbours)\n",
    "    elif LOSS_TYPE == \"original\":\n",
    "        return loss_normDiff(preds, TARGET)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown LOSS_TYPE\")\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "# ---------- run -------------------------------------------------------------\n",
    "print(\"JAX backend:\", jax.default_backend().upper(),\n",
    "      \"| lattice:\", C.LATTICE,\n",
    "      f\"| N={C.N_SITES}, Np={C.N_PART}\")\n",
    "\n",
    "state = create_state(jax.random.PRNGKey(42))\n",
    "for epoch in range(3*C.EPOCHS):\n",
    "    state, loss = train_step(state)\n",
    "    if epoch % C.PRINT_EVERY == 0:\n",
    "        print(f\"epoch {epoch:4d}  loss = {float(loss):.4e}\")\n",
    "\n",
    "# ---------- output ----------------------------------------------------------\n",
    "coeffs = model.apply(state.params, OCC, LAM, train=False)\n",
    "print(f\"\\nFirst {C.N_PRINT} coefficients for (t,V)=({C.T_HOP},{0.0}):\")\n",
    "for i in range(min(C.N_PRINT, len(basis))):\n",
    "    n_str = ''.join(str(int(b)) for b in OCC[i])\n",
    "    re, im = map(float, coeffs[i])\n",
    "    print(f\"|{n_str}⟩ → {re:+.5f} {im:+.5f} i\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00292afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ff6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Hamiltonian for the non-interacting 1D chain of fermions\n",
    "Array = jax.Array\n",
    "def H_builder(C) -> Array:\n",
    "    import itertools \n",
    "    from scipy.sparse import lil_matrix, csr_matrix \n",
    "    Ns              = C.N_SITES          # number of sites of the 1D chain\n",
    "    Nparticelle     = C.N_PART    # number of particles\n",
    "\n",
    "    # basis vectors  \n",
    "    klattice = 2*np.pi/Ns * np.arange(Ns)                                               # k lattice points  \n",
    "    linking_table = {jr: [np.mod(jr-1, Ns), np.mod(jr+1, Ns)] for jr in range(Ns)}      # 1D linking table \n",
    "    # generate Hilbert space \n",
    "    def n2occ(n,Nsize):\n",
    "        binr=np.binary_repr(n,width=Nsize) \n",
    "        return binr\n",
    "    def generate_combinations(N,Np):\n",
    "        \"\"\" Generate all combinations of Np particles from N available positions, returning them as binary numbers. \"\"\"\n",
    "        combinations = itertools.combinations(range(N), Np)\n",
    "        results = []\n",
    "        for comb in combinations:\n",
    "            binary_num = 0  # Start with an empty binary number (all zeros)\n",
    "            for i in comb:\n",
    "                binary_num |= (1 << i)  # Set the bit at position i\n",
    "            results.append(binary_num)  # Append the binary number to the results\n",
    "        return results\n",
    "    ##### Nparticelle number of particles specified previously >> N=2*Ns \n",
    "    combinations = generate_combinations(N=Ns,Np=Nparticelle)\n",
    "    Nstates = len(combinations)\n",
    "    states = np.arange(Nstates)\n",
    "    state2ind = dict(zip(combinations,states))\n",
    "    print(\"Number of states =\",Nstates)  \n",
    "    ###### hopping \n",
    "    def hop(string,j1,j2): # hopping \n",
    "        Ncount = sum( int(string[j]) for j in range(j2) ) + sum( int(string[j]) for j in range(j1) )  \n",
    "        if j2 >= j1: \n",
    "            hop_sign = Ncount\n",
    "        else: \n",
    "            hop_sign = 1 + Ncount \n",
    "        # distruggo in j2 \n",
    "        tmp = string \n",
    "        tmp = \"o\"+tmp+\"o\" \n",
    "        tmp = tmp[:1+j2]+\"0\"+tmp[2+j2:] \n",
    "        tmp_p = tmp[1:-1] \n",
    "        # creo in j1 \n",
    "        tmp_p = \"o\"+tmp_p+\"o\" \n",
    "        tmp_p = tmp_p[:1+j1]+\"1\"+tmp_p[2+j1:]  \n",
    "        out = tmp_p[1:-1]\n",
    "        return out, (-1)**hop_sign\n",
    "    # Build the Hamiltonian\n",
    "    Htunnel = lil_matrix((Nstates, Nstates), dtype=np.float64)   # LIL format for easy assignment\n",
    "    #Hint    = lil_matrix((Nstates, Nstates), dtype=np.float64)   # LIL format for easy assignment \n",
    "    #Hloc    = lil_matrix((Nstates, Nstates), dtype=np.float64)   # LIL format for easy assignment\n",
    "    for key, index in state2ind.items():\n",
    "        string = n2occ(n=key,Nsize=Ns)  \n",
    "        occ_vals = np.array([int(bit) for bit in string])  \n",
    "        posAH = np.where(occ_vals == 1)[0]\n",
    "        #posAH_odd = posAH[posAH % 2 == 1]\n",
    "        #Hloc[index,index] = len(posAH_odd)  # number of odd particles in the state \n",
    "        for xx in posAH: \n",
    "            jxx = linking_table[xx]\n",
    "            #Pxx = [ occ_vals[j] for j in jxx]\n",
    "            # print( xx , f'linking table, {jxx}', f'Pxx {Pxx}' , string , Pxx.count(1) )\n",
    "            #Hint[index,index] += Pxx.count(1)/2 \n",
    "            for yy in jxx:  \n",
    "                if occ_vals[yy] == 0: \n",
    "                    string_new, segno = hop(string=string,j1=yy,j2=xx)\n",
    "                    # print( xx , yy , f'new string {string_new}, sign {segno}' )\n",
    "                    occ_new = np.array([int(bit) for bit in string_new])  \n",
    "                    key_new = int(string_new,2) \n",
    "                    index_new = state2ind[key_new] \n",
    "                    Htunnel[index_new,index] += -segno\n",
    "    # Convert LIL matrix to dense numpy array, then to jax array\n",
    "    H_dense = np.array(Htunnel.todense(), dtype=np.float32)\n",
    "    H_jax = jnp.array(H_dense)\n",
    "    return H_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90bec997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler_jax.py\n",
    "\"\"\"\n",
    "Metropolis–Hastings sampler for spinless fermion Fock states at fixed N.\n",
    "Targets p(σ) ∝ |ψ(σ; λ)|^2 using your trained JAX model.\n",
    "\n",
    "API:\n",
    "  occ_batch = sample_occ_batch(\n",
    "      model_apply, params, lam_vec, L, N,\n",
    "      num_samples=4096, burn_in=1024, thin=4, n_chains=16, rng_seed=0)\n",
    "\n",
    "where\n",
    "  • model_apply(params, occ, lam, train=False) -> (B, 2) [Re, Im]\n",
    "  • lam_vec is a 1D array, e.g. [t, V, N]  or  [t, V]\n",
    "  • returns occ_batch as int32 array of shape (num_samples, L)\n",
    "\"\"\"\n",
    "\n",
    "from typing import Callable, Tuple\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "# ------------------------- utilities ----------------------------------------\n",
    "def _random_occ(L: int, N: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"Random bitstring with exactly N ones among L sites.\"\"\"\n",
    "    occ = np.zeros(L, dtype=np.int8)\n",
    "    occ[rng.choice(L, size=N, replace=False)] = 1\n",
    "    return occ\n",
    "\n",
    "def _propose_exchange(occ: np.ndarray, rng: np.random.Generator) -> Tuple[np.ndarray, int, int]:\n",
    "    \"\"\"\n",
    "    Propose a number-conserving move: pick one occupied site i and one empty site j and swap.\n",
    "    Returns (new_occ, i, j).  If all 0s or 1s (should not happen), returns copy.\n",
    "    \"\"\"\n",
    "    ones  = np.flatnonzero(occ == 1)\n",
    "    zeros = np.flatnonzero(occ == 0)\n",
    "    if len(ones) == 0 or len(zeros) == 0:\n",
    "        return occ.copy(), -1, -1\n",
    "    i = int(rng.choice(ones))\n",
    "    j = int(rng.choice(zeros))\n",
    "    new_occ = occ.copy()\n",
    "    new_occ[i] = 0\n",
    "    new_occ[j] = 1\n",
    "    return new_occ, i, j\n",
    "\n",
    "@jax.jit\n",
    "def _coeffs_to_probs(coeff_ri: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"(B,2)->(B,) probabilities |ψ|^2 with small epsilon for stability.\"\"\"\n",
    "    psi = coeff_ri[:, 0] + 1j * coeff_ri[:, 1]\n",
    "    return (jnp.abs(psi) ** 2 + 1e-30).real\n",
    "\n",
    "def _eval_probs(model_apply, params, occ_batch: np.ndarray, lam_vec: jnp.ndarray) -> np.ndarray:\n",
    "    \"\"\"Evaluate |ψ|^2 for a batch of integer 0/1 arrays (np) at fixed λ.\"\"\"\n",
    "    occ_j = jnp.asarray(occ_batch, dtype=jnp.int32)\n",
    "    lam_b = jnp.tile(lam_vec[None, :], (occ_j.shape[0], 1))\n",
    "    coeff = model_apply(params, occ_j, lam_b, train=False)        # (B,2)\n",
    "    probs = _coeffs_to_probs(coeff)                               # (B,)\n",
    "    # IMPORTANT: return a WRITEABLE NumPy array\n",
    "    return np.array(probs, dtype=np.float64, copy=True)\n",
    "\n",
    "\n",
    "# ------------------------- main sampler -------------------------------------\n",
    "def sample_occ_batch(model_apply: Callable,\n",
    "                     params,\n",
    "                     lam_vec: jnp.ndarray,\n",
    "                     L: int,\n",
    "                     N: int,\n",
    "                     num_samples: int = 4096,\n",
    "                     burn_in: int = 1024,\n",
    "                     thin: int = 4,\n",
    "                     n_chains: int = 16,\n",
    "                     rng_seed: int = 0) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Return an array of shape (num_samples, L) with entries in {0,1},\n",
    "    distributed approximately as |ψ(σ;λ)|^2, using n_chains independent MH chains.\n",
    "    \"\"\"\n",
    "    assert 0 <= N <= L\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    # allocate per-chain state\n",
    "    chains = np.stack([_random_occ(L, N, rng) for _ in range(n_chains)], axis=0)  # (C, L)\n",
    "    probs  = _eval_probs(model_apply, params, chains, lam_vec)                    # (C,)\n",
    "    if not probs.flags.writeable:\n",
    "        probs = probs.copy()\n",
    "\n",
    "    # how many samples per chain (ceil)\n",
    "    per_chain = (num_samples + n_chains - 1) // n_chains\n",
    "    samples = []\n",
    "\n",
    "    total_steps = burn_in + thin * per_chain\n",
    "    for step in range(total_steps):\n",
    "        # Propose for all chains in parallel (Python level)\n",
    "        props = []\n",
    "        for c in range(n_chains):\n",
    "            occ_new, _, _ = _propose_exchange(chains[c], rng)\n",
    "            props.append(occ_new)\n",
    "        props = np.stack(props, axis=0)                                # (C, L)\n",
    "\n",
    "        probs_new = _eval_probs(model_apply, params, props, lam_vec)   # (C,)\n",
    "        # MH accept\n",
    "        accept_ratio = probs_new / (probs + 1e-300)\n",
    "        accept = rng.random(n_chains) < np.minimum(1.0, accept_ratio)\n",
    "\n",
    "        # update chains & probs\n",
    "        chains[accept] = props[accept]\n",
    "        probs[accept]  = probs_new[accept]\n",
    "\n",
    "        # record (after burn-in) with thinning\n",
    "        if step >= burn_in and ((step - burn_in) % thin == 0):\n",
    "            samples.append(chains.copy())\n",
    "\n",
    "    # collect & trim\n",
    "    if len(samples) == 0:\n",
    "        # edge case: too small total_steps\n",
    "        return jnp.asarray(chains[:num_samples], dtype=jnp.int32)\n",
    "\n",
    "    samples = np.concatenate(samples, axis=0)           # (~per_chain, C, L)\n",
    "    samples = samples.reshape(-1, L)                    # (C*~, L)\n",
    "    samples = samples[:num_samples]                     # exact M\n",
    "    return jnp.asarray(samples, dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3f7094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states = 70\n"
     ]
    }
   ],
   "source": [
    "## Given ground state wavefunction, what is ground state energy?\n",
    "import observables.energy as E\n",
    "import sampler.sampler_jax as sampler\n",
    "# (Assumes you have (i) a sampler that draws σ ~ |ψ|^2 and (ii) H for the basis.)\n",
    "\n",
    "model_apply = lambda params, occ, lam, train=False: \\\n",
    "    model.apply(params, occ, lam, train=train)\n",
    "\n",
    "L, N, t = C.N_SITES, C.N_PART, C.T_HOP\n",
    "\n",
    "# 1) Basis + index map (must match H ordering!) (For ED and beyond)\n",
    "basis_masks = enumerate_fock(L, N)\n",
    "occ_basis   = jnp.array([mask_to_array(m, L) for m in basis_masks], dtype=jnp.int32)\n",
    "state_index = E.build_state_index([int(m) for m in basis_masks])\n",
    "\n",
    "# 2) Hamiltonian for appropriate basis\n",
    "H = H_builder(C) # can change this but currently C is set to npart = 4 and L = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36613162",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to restart the Kernel. \n",
      "\u001b[1;31mrequest to http://127.0.0.1:8890/api/kernels/15b22f8e-4459-4d32-800a-e859b3ade27e/restart?1755106382059 failed, reason: read ECONNRESET. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# sampler/sampler_jax.py  (pure JAX MH sampler; fixed-N occupations)\n",
    "from typing import Callable, Tuple\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from functools import partial\n",
    "\n",
    "EPS = 1e-30\n",
    "\n",
    "def _coeffs_to_probs(coeff_ri: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"(B,2)->(B,) probabilities |ψ|^2 (float32), numerically safe.\"\"\"\n",
    "    psi = coeff_ri[..., 0] + 1j * coeff_ri[..., 1]\n",
    "    return (jnp.abs(psi)**2 + EPS).real.astype(jnp.float32)\n",
    "\n",
    "def _eval_probs(model_apply: Callable, params, occ_batch: jnp.ndarray, lam_vec: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate |ψ|^2 for a batch of 0/1 int32 arrays at fixed λ.\n",
    "    occ_batch: (C, L) int32, lam_vec: (K,) float32\n",
    "    returns: (C,) float32\n",
    "    \"\"\"\n",
    "    C = occ_batch.shape[0]\n",
    "    lam_b = jnp.broadcast_to(lam_vec, (C, lam_vec.shape[-1]))\n",
    "    coeff = model_apply(params, occ_batch, lam_b, train=False)  # (C, 2), JAX op\n",
    "    return _coeffs_to_probs(coeff)\n",
    "\n",
    "def _forward(model, params, occ_batch, lam_batch, *, train: bool):\n",
    "    # shapes:\n",
    "    #   occ_batch: (B, L) int32 in {0,1}\n",
    "    #   lam_batch: (B, K) float32\n",
    "    return model.apply(params, occ_batch, lam_batch, train=train)  # -> (B,2)\n",
    "\n",
    "# 2) make a jitted, batched apply with 'train' as a static arg so it compiles once\n",
    "def make_apply_batched(model):\n",
    "    f = partial(_forward, model)\n",
    "    return jax.jit(f, static_argnames=('train',))\n",
    "\n",
    "apply_batched = make_apply_batched(model)\n",
    "\n",
    "@partial(jax.jit, static_argnames=('train',))\n",
    "def eval_probs(params, occ_batch, lam_batch, *, train: bool):\n",
    "    coeff = apply_batched(params, occ_batch, lam_batch, train=train)   # (B,2)\n",
    "    return _coeffs_to_probs(coeff)  \n",
    "\n",
    "def _random_occ(key: jax.Array, L: int, N: int) -> jnp.ndarray:\n",
    "    \"\"\"Random bitstring with exactly N ones among L using Gumbel-top-k.\"\"\"\n",
    "    g = jax.random.gumbel(key, (L,))\n",
    "    # take top-N positions\n",
    "    _, idx = lax.top_k(g, N)               # idx: (N,)\n",
    "    occ = jnp.zeros((L,), dtype=jnp.int32)\n",
    "    occ = occ.at[idx].set(1)\n",
    "    return occ\n",
    "\n",
    "def _propose_exchange(keys: Tuple[jax.Array, jax.Array], occ: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Propose a number-conserving move by choosing one occupied and one empty site\n",
    "    uniformly at random (via Gumbel-max) and swapping them.\n",
    "    keys: (key_occ, key_emp)\n",
    "    occ: (L,) int32 in {0,1}\n",
    "    \"\"\"\n",
    "    key_occ, key_emp = keys\n",
    "    L = occ.shape[0]\n",
    "    # sample indices uniformly among occupied / empty positions\n",
    "    g1 = jax.random.gumbel(key_occ, (L,))\n",
    "    g0 = jax.random.gumbel(key_emp, (L,))\n",
    "    i = jnp.argmax(jnp.where(occ.astype(bool), g1, -jnp.inf))     # occupied index\n",
    "    j = jnp.argmax(jnp.where(~occ.astype(bool), g0, -jnp.inf))    # empty index\n",
    "    # swap bits\n",
    "    new = occ\n",
    "    new = new.at[i].set(0)\n",
    "    new = new.at[j].set(1)\n",
    "    return new\n",
    "\n",
    "def sample_occ_batch(model_apply: Callable,\n",
    "                     params,\n",
    "                     lam_vec: jnp.ndarray,\n",
    "                     L: int,\n",
    "                     N: int,\n",
    "                     num_samples: int = 4096,\n",
    "                     burn_in: int = 1024,\n",
    "                     thin: int = 4,\n",
    "                     n_chains: int = 16,\n",
    "                     rng_seed: int = 0) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Draw (num_samples, L) configurations approximately from |ψ(σ;λ)|^2\n",
    "    by running n_chains independent MH chains on the GPU.\n",
    "\n",
    "    model_apply(params, occ, lam, train=False) -> (B,2) [Re, Im] must be pure JAX.\n",
    "    lam_vec: (K,) float32, e.g. [t, V, N]\n",
    "    \"\"\"\n",
    "    assert 0 <= N <= L\n",
    "\n",
    "    # --- PRNG setup\n",
    "    key = jax.random.PRNGKey(rng_seed)\n",
    "    k_init, k_scan = jax.random.split(key)\n",
    "\n",
    "    # --- initialize chains (C, L) with exactly N ones each (vectorized)\n",
    "    init_keys = jax.random.split(k_init, n_chains)\n",
    "    chains0 = jax.vmap(_random_occ, in_axes=(0, None, None))(init_keys, L, N)  # (C, L)\n",
    "\n",
    "    # --- initial probabilities on device\n",
    "    probs0 = _eval_probs(model_apply, params, chains0, lam_vec)               # (C,)\n",
    "\n",
    "    # --- how many samples per chain to hit num_samples exactly\n",
    "    per_chain = (num_samples + n_chains - 1) // n_chains\n",
    "    total_steps = burn_in + thin * per_chain\n",
    "\n",
    "    # pre-allocate save buffer\n",
    "    saved0 = jnp.zeros((per_chain, n_chains, L), dtype=jnp.int32)\n",
    "    save_idx0 = jnp.array(0, dtype=jnp.int32)\n",
    "\n",
    "    \"\"\" def one_step(carry, step_idx):\n",
    "        \n",
    "        #Single MH step across all chains, all on-device.\n",
    "        #Saves states after burn-in at thinning interval into a ring buffer.\n",
    "        \n",
    "        key, chains, probs, save_idx, saved = carry\n",
    "        key, k1, k2, k3 = jax.random.split(key, 4)\n",
    "\n",
    "        # propose for all chains in parallel (no Python loop)\n",
    "        g1 = jax.random.gumbel(k1, (n_chains, L))\n",
    "        g0 = jax.random.gumbel(k2, (n_chains, L))\n",
    "        i_occ = jnp.argmax(jnp.where(chains == 1, g1, -jnp.inf), axis=1)  # (C,)\n",
    "        i_emp = jnp.argmax(jnp.where(chains == 0, g0, -jnp.inf), axis=1)  # (C,)\n",
    "        ar = jnp.arange(n_chains, dtype=jnp.int32)\n",
    "\n",
    "        props = chains.at[ar, i_occ].set(0)\n",
    "        props = props.at[ar, i_emp].set(1)\n",
    "\n",
    "        probs_new = _eval_probs(model_apply, params, props, lam_vec)         # (C,)\n",
    "        accept_ratio = probs_new / (probs + EPS)\n",
    "        u = jax.random.uniform(k3, (n_chains,))\n",
    "        accept = u < jnp.minimum(1.0, accept_ratio)\n",
    "\n",
    "        chains_new = jnp.where(accept[:, None], props, chains)\n",
    "        probs_new2 = jnp.where(accept, probs_new, probs)\n",
    "\n",
    "        # save if we're past burn-in and on a thinning step\n",
    "        keep = jnp.logical_and(step_idx >= burn_in,\n",
    "                               jnp.equal(jnp.remainder(step_idx - burn_in, thin), 0))\n",
    "        saved = lax.cond(\n",
    "            keep,\n",
    "            lambda buf: buf.at[save_idx].set(chains_new),\n",
    "            lambda buf: buf,\n",
    "            saved\n",
    "        )\n",
    "        save_idx = save_idx + keep.astype(jnp.int32)\n",
    "\n",
    "        carry_out = (key, chains_new, probs_new2, save_idx, saved)\n",
    "        return carry_out, None\n",
    "\n",
    "    # --- run the MH scan\n",
    "    carry0 = (k_scan, chains0, probs0, save_idx0, saved0)\n",
    "    carry_out, _ = lax.scan(one_step, carry0, jnp.arange(total_steps))\n",
    "    _, chains_f, _, _, saved = carry_out \"\"\"\n",
    "\n",
    "    # --- flatten samples and return exactly num_samples\n",
    "    # saved: (per_chain, C, L) -> (per_chain*C, L)\n",
    "    @partial(jax.jit, donate_argnums=(1,2,4))  # donate chains, probs, saved\n",
    "    def run_mh_loop(key, chains0, probs0, saved0, save_idx0, lam_batch_C, total_steps, burn_in, thin):\n",
    "        def one_step(carry, step_idx):\n",
    "            key, chains, probs, save_idx, saved = carry\n",
    "            key, k1, k2, k3 = jax.random.split(key, 4)\n",
    "            C, L = chains.shape\n",
    "\n",
    "            # propose via masked-gumbel argmax (same as before)\n",
    "            g1 = jax.random.gumbel(k1, (C, L))\n",
    "            g0 = jax.random.gumbel(k2, (C, L))\n",
    "            i_occ = jnp.argmax(jnp.where(chains == 1, g1, -jnp.inf), axis=1)\n",
    "            i_emp = jnp.argmax(jnp.where(chains == 0, g0, -jnp.inf), axis=1)\n",
    "            ar = jnp.arange(C, dtype=jnp.int32)\n",
    "            props = chains.at[ar, i_occ].set(0)\n",
    "            props = props.at[ar, i_emp].set(1)\n",
    "\n",
    "            # *** batched jitted model eval here ***\n",
    "            probs_new = eval_probs(state.params, props, lam_batch_C, train=False)\n",
    "            accept_ratio = probs_new / (probs + 1e-30)\n",
    "            u = jax.random.uniform(k3, (C,))\n",
    "            accept = u < jnp.minimum(1.0, accept_ratio)\n",
    "\n",
    "            chains_new = jnp.where(accept[:, None], props, chains)\n",
    "            probs_new2 = jnp.where(accept, probs_new, probs)\n",
    "\n",
    "            keep = jnp.logical_and(step_idx >= burn_in,\n",
    "                                (step_idx - burn_in) % thin == 0)\n",
    "            saved = lax.cond(\n",
    "                keep,\n",
    "                lambda buf: buf.at[save_idx].set(chains_new),\n",
    "                lambda buf: buf,\n",
    "                saved\n",
    "            )\n",
    "            save_idx = save_idx + keep.astype(jnp.int32)\n",
    "            return (key, chains_new, probs_new2, save_idx, saved), None\n",
    "\n",
    "        (key_f, chains_f, probs_f, save_idx_f, saved_f), _ = lax.scan(\n",
    "            one_step,\n",
    "            (key, chains0, probs0, save_idx0, saved0),\n",
    "            jnp.arange(total_steps),\n",
    "        )\n",
    "        return saved_f\n",
    "    \n",
    "    def make_lam_batch(lam_vec, B):\n",
    "        lam_vec = jnp.asarray(lam_vec, dtype=jnp.float32)\n",
    "        return jnp.broadcast_to(lam_vec, (B, lam_vec.shape[-1]))\n",
    "\n",
    "    lam_batch_C = make_lam_batch(lam_vec, C)\n",
    "\n",
    "    saved = run_mh_loop(key, chains0, probs0, saved0, save_idx0, lam_batch_C, total_steps, burn_in, thin)\n",
    "\n",
    "\n",
    "    samples = saved.reshape(-1, L)[:num_samples]\n",
    "    return samples.astype(jnp.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "511ab997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E0(model, MC) = -4.187276840209961\n"
     ]
    }
   ],
   "source": [
    "# 3) Draw M samples σ ~ |ψ|^2  (use your sampler; pseudo-code shown)\n",
    "M = 1024\n",
    "lam_vec   = jnp.array([2*t, 0.0, float(N)], dtype=jnp.float32) # non-interacting, so V=0 # C.T_HOP is 0.5\n",
    "lam_batch = jnp.tile(lam_vec, (M, 1))\n",
    "\n",
    "# 1) Draw configurations σ ~ |ψ|^2\n",
    "occ_batch = sample_occ_batch(model_apply, state.params, lam_vec,\n",
    "                             L=L, N=N,\n",
    "                             num_samples=M, burn_in=256, thin=4,\n",
    "                             n_chains=32, rng_seed=0)\n",
    "\n",
    "def model_fn(occ, lam):\n",
    "    coeff = model_apply(state.params, occ, lam, train=False)  # (B,2)\n",
    "    return (coeff[:, 0] + 1j * coeff[:, 1]).astype(jnp.complex64)\n",
    "\n",
    "# 4) Monte Carlo energy (module function averages local energies)\n",
    "E_mc = E.expectation_local_energy(model=model_fn,\n",
    "                      occ_batch=occ_batch,\n",
    "                      params_batch=lam_batch,\n",
    "                      H=H,\n",
    "                      occ_basis=occ_basis,\n",
    "                      state_index=state_index)\n",
    "print(\"E0(model, MC) =\", float(E_mc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b808a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'jaxlib._jax.ArrayImpl'>, [[ 0. -1.  0. ...  0.  0.  0.]\n [-1.  0. -1. ...  0.  0.  0.]\n [ 0. -1.  0. ...  0.  0.  0.]\n ...\n [ 0.  0.  0. ...  0. -1.  0.]\n [ 0.  0.  0. ... -1.  0. -1.]\n [ 0.  0.  0. ...  0. -1.  0.]]. The error was:\nTypeError: unhashable type: 'jaxlib._jax.ArrayImpl'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     20\u001b[39m batched_local_energy = jax.jit(\n\u001b[32m     21\u001b[39m     jax.vmap(local_energy_single, in_axes=(\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[32m     22\u001b[39m     static_argnames=(\u001b[33m'\u001b[39m\u001b[33mH\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mocc_basis\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mstate_index\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     23\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Warmup compile (important to separate compile vs run time):\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m e_loc_temp = \u001b[43mbatched_local_energy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocc_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocc_basis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_index\u001b[49m\u001b[43m)\u001b[49m.block_until_ready()\n",
      "\u001b[31mValueError\u001b[39m: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'jaxlib._jax.ArrayImpl'>, [[ 0. -1.  0. ...  0.  0.  0.]\n [-1.  0. -1. ...  0.  0.  0.]\n [ 0. -1.  0. ...  0.  0.  0.]\n ...\n [ 0.  0.  0. ...  0. -1.  0.]\n [ 0.  0.  0. ... -1.  0. -1.]\n [ 0.  0.  0. ...  0. -1.  0.]]. The error was:\nTypeError: unhashable type: 'jaxlib._jax.ArrayImpl'\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "jax.config.update(\"jax_enable_x64\", False)\n",
    "def psi_fn(params, occ, lam):\n",
    "    # vectorizes over batch outside; single sample here\n",
    "    c = model_apply(params, occ, lam, train=False)  # shape (2,)\n",
    "    return (c[0] + 1j*c[1]).astype(jnp.complex64)\n",
    "\n",
    "# Local energy for a **single** configuration; must be pure JAX\n",
    "def local_energy_single(occ, lam, params, H, occ_basis, state_index):\n",
    "    # --- IMPORTANT ---\n",
    "    # * No Python loops\n",
    "    # * No numpy/scipy\n",
    "    # * H must be represented so its actions are JAX ops (or precomputed arrays)\n",
    "    psi = psi_fn(params, occ, lam)\n",
    "    # ... compute E_loc purely with jnp / lax ...\n",
    "    # placeholder:\n",
    "    return jnp.real(psi) * 0.0  # replace with your jax-only E_loc\n",
    "\n",
    "# Batch it, then jit it. Mark python/structure args static so XLA doesn't recompile.\n",
    "batched_local_energy = jax.jit(\n",
    "    jax.vmap(local_energy_single, in_axes=(0,0,None,None,None,None)),\n",
    "    static_argnames=('H','occ_basis','state_index')\n",
    ")\n",
    "\n",
    "# Warmup compile (important to separate compile vs run time):\n",
    "e_loc_temp = batched_local_energy(occ_batch, lam_batch, state.params, H, occ_basis, state_index).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7ba273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E0(model, MC) = -4.828277587890625\n"
     ]
    }
   ],
   "source": [
    "# Larger M for better accuracy, takes far longer and is marginally better\n",
    "M = 4096\n",
    "lam_vec   = jnp.array([t, 0.0, float(N)], dtype=jnp.float32) # non-interacting, so V=0\n",
    "lam_batch = jnp.tile(lam_vec, (M, 1))\n",
    "\n",
    "# 1) Draw configurations σ ~ |ψ|^2\n",
    "occ_batch = sample_occ_batch(model_apply, state.params, lam_vec,\n",
    "                             L=L, N=N,\n",
    "                             num_samples=M, burn_in=1024, thin=4,\n",
    "                             n_chains=32, rng_seed=0)\n",
    "\n",
    "def model_fn(occ, lam):\n",
    "    coeff = model_apply(state.params, occ, lam, train=False)  # (B,2)\n",
    "    return (coeff[:, 0] + 1j * coeff[:, 1]).astype(jnp.complex64)\n",
    "\n",
    "# 4) Monte Carlo energy (module function averages local energies)\n",
    "E_mc = E.expectation_local_energy(model=model_fn,\n",
    "                      occ_batch=occ_batch,\n",
    "                      params_batch=lam_batch,\n",
    "                      H=H,\n",
    "                      occ_basis=occ_basis,\n",
    "                      state_index=state_index)\n",
    "print(\"E0(model, MC) =\", float(E_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4c3862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E0(H) = -4.828428745269775\n"
     ]
    }
   ],
   "source": [
    "# use the SAME H you pass to expectation_local_energy (straight exact diagonalization)\n",
    "evals = jnp.linalg.eigvalsh(H)      # H is (Hdim,Hdim) in N-sector\n",
    "E_exact_from_H = float(evals[0])\n",
    "print(\"E0(H) =\", E_exact_from_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6448d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E0(Rayleigh) = -3.919802665710449\n"
     ]
    }
   ],
   "source": [
    "# Rayleigh quotient method\n",
    "# psi_full on the full basis used to build H\n",
    "psi   = model_fn(occ, lam)\n",
    "E_ray = float((jnp.vdot(psi, H @ psi) / (jnp.vdot(psi, psi) + 1e-12)).real) ## Hilbert space needs to be same for model and H\n",
    "# Throws error because model is npart = 3 and L = 8 while H is npart = 4 and L = 8\n",
    "print(\"E0(Rayleigh) =\", E_ray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b93592d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E0(model, MC) = -3.914226531982422\n"
     ]
    }
   ],
   "source": [
    "E_mc = float(E.expectation_local_energy(\n",
    "              model=model_fn, occ_batch=occ_batch,\n",
    "              params_batch=lam_batch, H=H,\n",
    "              occ_basis=occ_basis, state_index=state_index).real)\n",
    "print(\"E0(model, MC) =\", E_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e70d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E0(model, local energy) = -1.9571131467819214\n"
     ]
    }
   ],
   "source": [
    "# local_energy_1d.py  (drop-in helper)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def local_energy_1d_tb(model_fn, occ_batch: jnp.ndarray, lam_batch: jnp.ndarray,\n",
    "                       t: float, bc: str = \"pbc\") -> jnp.ndarray:\n",
    "    M, L = occ_batch.shape\n",
    "    N = jnp.sum(occ_batch[0])  # fixed-N sampler\n",
    "    # base amplitudes\n",
    "    psi = model_fn(occ_batch, lam_batch)  # (M,)\n",
    "    eps = 1e-30\n",
    "\n",
    "    # helper to flip i->j\n",
    "    def move(occ_np, i, j):\n",
    "        occ2 = occ_np.copy()\n",
    "        occ2[i] = 0; occ2[j] = 1\n",
    "        return occ2\n",
    "\n",
    "    # JW sign for wrap hops:\n",
    "    jw = 1.0 if (int(N) % 2 == 1) else -1.0\n",
    "    bc_phase = 1.0 if bc == \"pbc\" else -1.0  # add a twist if you want APBC\n",
    "    wrap_sign = jw * bc_phase\n",
    "\n",
    "    E_loc = np.zeros(M, dtype=np.complex64)\n",
    "\n",
    "    # loop in Python (clear & reliable; vectorise later if needed)\n",
    "    for m in range(M):\n",
    "        occ = np.asarray(occ_batch[m])\n",
    "        lam = np.asarray(lam_batch[m])\n",
    "        denom = complex(psi[m])\n",
    "        if abs(denom) < eps:\n",
    "            continue\n",
    "        acc = 0.0 + 0.0j\n",
    "        for i in range(L):\n",
    "            if occ[i] == 0: \n",
    "                continue\n",
    "            # hop right\n",
    "            j = (i + 1) % L\n",
    "            if occ[j] == 0:\n",
    "                occ2 = move(occ, i, j)\n",
    "                # wrap sign if i->j crosses boundary\n",
    "                sgn = wrap_sign if (i == L-1 and j == 0) else 1.0\n",
    "                num = complex(model_fn(jnp.asarray(occ2[None,:], dtype=jnp.int32),\n",
    "                                       jnp.asarray(lam[None,:], dtype=jnp.float32))[0])\n",
    "                acc += -t * sgn * (num / denom)\n",
    "            # hop left\n",
    "            j = (i - 1) % L\n",
    "            if occ[j] == 0:\n",
    "                occ2 = move(occ, i, j)\n",
    "                sgn = wrap_sign if (i == 0 and j == L-1) else 1.0\n",
    "                num = complex(model_fn(jnp.asarray(occ2[None,:], dtype=jnp.int32),\n",
    "                                       jnp.asarray(lam[None,:], dtype=jnp.float32))[0])\n",
    "                acc += -t * sgn * (num / denom)\n",
    "        E_loc[m] = acc\n",
    "    return jnp.asarray(E_loc)\n",
    "\n",
    "E_loc = local_energy_1d_tb(model_fn, occ_batch, lam_batch, C.T_HOP, bc=\"pbc\")\n",
    "E_mc  = float(jnp.mean(E_loc).real)\n",
    "print(\"E0(model, local energy) =\", E_mc) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e8c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states = 56\n",
      "E0(H) = -4.828426837921143\n"
     ]
    }
   ],
   "source": [
    "## Old version sketch of how to get local energy \n",
    "\"\"\" ## Given ground state wavefunction, what is ground state energy?\n",
    "import observables.energy as E\n",
    "import sampler.mcmc as sampler\n",
    "# (Assumes you have (i) a sampler that draws σ ~ |ψ|^2 and (ii) H for the basis.)\n",
    "\n",
    "model_apply = lambda params, occ, lam, train=False: \\\n",
    "    model.apply(params, occ, lam, train=train)\n",
    "\n",
    "# 1) Basis + index map (must match H ordering!)\n",
    "basis_masks = enumerate_fock(C.N_SITES, C.N_PART)\n",
    "occ_basis   = jnp.array([mask_to_array(m, C.N_SITES) for m in basis_masks], dtype=jnp.int32)\n",
    "state_index = E.build_state_index([int(m) for m in basis_masks])\n",
    "\n",
    "# 2) Your Hamiltonian for that basis\n",
    "# H = your_ed_builder(...)\n",
    "\n",
    "# 3) Draw M samples σ ~ |ψ|^2  (use your sampler; pseudo-code shown)\n",
    "M = 4096\n",
    "lam_vec   = jnp.array([C.T_HOP, 0.0, C.N_PART], dtype=jnp.float32) # non-interacting, so V=0\n",
    "lam_batch = jnp.tile(lam_vec, (M, 1))\n",
    "# occ_batch = sampler.sample(model, state.params, lam_vec, M)  # shape (M, N_sites)\n",
    "occ_batch = sampler.sample_chain(init_state: Array,\n",
    "                 log_prob_fn: model_apply,\n",
    "                 key: PRNGKey,\n",
    "                 n_samples: 4096,\n",
    "                 burn_in= 100,\n",
    "                 thin=1)\n",
    "\n",
    "# 4) Monte Carlo energy (module function averages local energies)\n",
    "E_mc = E.expectation_local_energy(model=model_apply,\n",
    "                      occ_batch=occ_batch,\n",
    "                      params_batch=lam_batch,\n",
    "                      H=H,\n",
    "                      occ_basis=occ_basis,\n",
    "                      state_index=state_index)\n",
    "print(\"E0(model, MC) =\", float(E_mc)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de63c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotrain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
