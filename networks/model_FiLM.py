# model.py
"""
λ-conditioned Lattice-PsiTransformer
--------------------------------
For every transformer block we inject a **FiLM** modulation whose scale/shift
vectors are generated by a tiny hyper-network g(λ).  The *effective*
weights θ(λ) change continuously with the Hamiltonian parameters.
"""
from typing import Sequence, Callable
import jax, jax.numpy as jnp
import flax.linen as nn

# --------------------------------------------------------------------------- #
#  FiLM and conditional LayerNorm                                             #
# --------------------------------------------------------------------------- #
class FiLM(nn.Module):
    d_model: int
    def setup(self):
        self.gen = nn.Dense(2 * self.d_model)          # λ → (γ,β)

    def __call__(self, h, lam):
        """
        h   : (B, L, D)   activation to be modulated
        lam : (B, C)      Hamiltonian parameters
        """
        gamma_beta = self.gen(lam)                     # (B, 2D)
        gamma, beta = jnp.split(gamma_beta, 2, axis=-1)
        gamma, beta = gamma[:, None, :], beta[:, None, :]
        return gamma * h + beta


class CondLayerNorm(nn.Module):
    d_model: int
    eps: float = 1e-6
    def setup(self):
        self.film = FiLM(self.d_model)

    def __call__(self, x, lam):
        mean = jnp.mean(x, axis=-1, keepdims=True)
        var  = jnp.mean((x - mean) ** 2, axis=-1, keepdims=True)
        norm = (x - mean) * jax.lax.rsqrt(var + self.eps)
        return self.film(norm, lam)                    # γ,β depend on λ

# --------------------------------------------------------------------------- #
#  Self-attention block with λ-FiLM                                            #
# --------------------------------------------------------------------------- #
class ResidualBlock(nn.Module):
    d_model: int
    n_heads: int
    mlp_dims: Sequence[int]
    def setup(self):
        self.ln1   = CondLayerNorm(self.d_model)
        self.attn  = nn.SelfAttention(
            num_heads=self.n_heads,
            dropout_rate=0.0)
        self.ln2   = CondLayerNorm(self.d_model)
        self.mlp   = [nn.Dense(m) for m in self.mlp_dims] + \
                     [nn.Dense(self.d_model)]
        self.act   = nn.gelu

    def __call__(self, x, lam, *, train: bool):
        # 1️ λ-modulated self-attention
        h = self.ln1(x, lam)
        x = x + self.attn(h, deterministic=not train)

        # 2️ λ-modulated position-wise MLP
        h = self.ln2(x, lam)
        for layer in self.mlp[:-1]:
            h = self.act(layer(h))
        h  = self.mlp[-1](h)
        return x + h                                   # second residual

# --------------------------------------------------------------------------- #
#  Cross-attention (unchanged except scalable-softmax)                        #
# --------------------------------------------------------------------------- #
def scalable_softmax(logits, axis=-1, s=0.5):
    n = logits.shape[axis]
    logits = logits * (s * jnp.log(float(n)))
    logits = logits - jnp.max(logits, axis=axis, keepdims=True)
    exp    = jnp.exp(logits)
    return exp / jnp.sum(exp, axis=axis, keepdims=True)

class CrossAttentionParams(nn.Module):
    d_model: int
    n_heads: int
    def setup(self):
        self.wq = nn.Dense(self.d_model, use_bias=False)
        self.wk = nn.Dense(self.d_model, use_bias=False)
        self.wv = nn.Dense(self.d_model, use_bias=False)
        self.wo = nn.Dense(self.d_model, use_bias=False)
        self.s  = self.param('sscale', lambda k: jnp.array(0.5))

    def __call__(self, sites, p_tok):
        # sites:(B,L,D)  p_tok:(B,1,D)
        B,L,_ = sites.shape
        q = self.wq(sites).reshape(B,L,self.n_heads,-1).transpose(0,2,1,3)
        k = self.wk(p_tok).reshape(B,1,self.n_heads,-1).transpose(0,2,1,3)
        v = self.wv(p_tok).reshape(B,1,self.n_heads,-1).transpose(0,2,1,3)
        logits = jnp.einsum('bhqd,bhkd->bhqk', q, k) / jnp.sqrt(k.shape[-1])
        attn   = scalable_softmax(logits, axis=-1, s=self.s)
        out    = jnp.einsum('bhqk,bhkd->bhqd', attn, v)
        out    = out.transpose(0,2,1,3).reshape(B, L, -1)
        return self.wo(out)

# --------------------------------------------------------------------------- #
#  Full λ-conditioned PsiFormer                                               #
# --------------------------------------------------------------------------- #
class LatticePsiFormer(nn.Module):
    n_sites: int
    d_model: int = 256
    depth: int   = 8
    n_heads: int = 8
    mlp_dims: Sequence[int] = (512,)
    @nn.compact
    def __call__(self, occ, lam, *, train: bool = False):
        """
        occ : (B,N) int32     Fock occupations
        lam : (B,C) float32   Hamiltonian parameters (t, V, N, …)
        """
        B,N = occ.shape
        tok = nn.Embed(2, self.d_model)(occ)
        pos = self.param('pos', nn.initializers.normal(0.02),
                         (1, self.n_sites, self.d_model))
        x   = tok + pos

        # Residual self-attention × depth
        for _ in range(self.depth):
            x = ResidualBlock(self.d_model, self.n_heads,
                              self.mlp_dims)(x, lam, train=train)

        # Cross-attention parameter token
        p_tok = nn.Dense(self.d_model)(lam)[..., None, :]   # (B,1,D)
        x = x + CrossAttentionParams(self.d_model,
                                     self.n_heads)(x, p_tok)

        # Pool & read-out
        h = nn.LayerNorm()(x.mean(axis=1))
        out = nn.Dense(2)(h)                                # (B,2)  Re, Im
        return out
